 #La historia de hadoop

_Comienzos_

### Pre-requisitos 📋
_EstasInspirándose en la computación en paralelo de Google, los programadores Mike Cafarella y Doug Cutting lanzaron la primera versión de Hadoop el 1 de abril de 2006. Se trata de una solución de código abierto que emplea la computación en paralelo para procesar y analizar volúmenes enormes de data. 
Cutting inició la investigación mientras trabajaba en Google, y la continuó al marcharse a Yahoo. Entonces se enmarcó en el proyecto de desarrollo de Nutch, motor de búsqueda de esta última compañía, este proyecto tenía problemas de escalabilidad, leyó el paper de Google y lo implemento, dando lugar a Hadoop y HDFS como proyectos de Apache. El proyecto de Nutch se dividió: la parte del rastreador web permaneció como Nutch y la porción de procesamiento y computación distribuida se convirtió en Hadoop (que lleva el nombre del elefante de juguete del hijo de Cutting). Poco después creo Cloudera._


## Desarrollo 🚀

_Hadoop se desarrolló como un modelo de procesamiento automático basado en computación en paralelo extrapolable a cualquier programa de procesamiento de grandes volúmenes de datos.
Hadoop es un framework de código abierto, esto es, que cualquier persona puede acceder a sus componentes de forma libre y gratuita, y los puede modificar y adaptar a las necesidades particulares del proyecto que esté desarrollando. Esto, unido a su eficacia, hace que, a día de hoy, siga siendo el sistema más empleado en Big Data.
El bajo costo del hardware básico hace que Hadoop sea muy útil para almacenar y combinar datos como transaccionales, redes sociales, sensores, máquinas, científicos, transmisiones de clics, etc. El almacenamiento de bajo costo le permite mantener información que no se considera actualmente crítica, pero con la posibilidad de que usted la analice después._


